{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import requests     #以后尽可能直接用request，不再用urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openPage(urlPage):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'my custom user agent', 'Cookie': 'haha'}    #加headers\n",
    "        html = requests.get(urlPage, headers = headers)\n",
    "        #bsObj = BeautifulSoup(html,\"html.parser\",from_encoding=\"gb18030\")\n",
    "        bsObj = BeautifulSoup(html.content)\n",
    "    except Exception as e:\n",
    "        print('except:', e)\n",
    "        if e == \"HTTP Error 404: Not Found\" or e == \"HTTP Error 445:\":\n",
    "            print(urlPage)\n",
    "        else:\n",
    "            time.sleep(3)\n",
    "            bsObj = getLinks(urlPage)\n",
    "    return bsObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinks(urlpage):\n",
    "    bsObj = openPage(urlpage)\n",
    "    weeklist = bsObj.find(\"div\",{\"class\":\"erji_list1\"}).find_all(\"li\")\n",
    "    for li in weeklist:\n",
    "        link = li.find(\"a\")['href'][1:]\n",
    "        if li not in week_dict.values():\n",
    "            week_dict[li.find(\"a\").get_text()] = \"http://www.chinaivdc.cn/cnic/zyzx/lgzb\" + li.find(\"a\")['href'][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_dict = {}\n",
    "url = \"http://www.chinaivdc.cn/cnic/zyzx/lgzb/index\"\n",
    "getLinks(url + \".htm\")    #首页\n",
    "for i in range(1,4):     #从第二页开始的页码\n",
    "    getLinks(url + \"_\" + str(i) + \".htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(40,53):\n",
    "    week_dict.pop(\"2009 第\" + str(j) + \"周\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "print(len(week_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.chinaivdc.cn/cnic/zyzx/lgzb/201811/t20181105_196899.htm\n",
      "except: 'NoneType' object has no attribute 'group'\n",
      "http://www.chinaivdc.cn/cnic/zyzx/lgzb/201810/t20181029_196698.htm\n",
      "except: 'NoneType' object has no attribute 'group'\n",
      "http://www.chinaivdc.cn/cnic/zyzx/lgzb/201810/t20181016_195103.htm\n",
      "except: 'NoneType' object has no attribute 'group'\n"
     ]
    }
   ],
   "source": [
    "ILIdata = []\n",
    "for k,v in week_dict.items():\n",
    "    try:\n",
    "        date = k\n",
    "        \n",
    "        headers = {'User-Agent': 'my custom user agent', 'Cookie': 'qwer'}\n",
    "        res = requests.get(v,headers = headers)\n",
    "        res.encoding= 'utf-8'\n",
    "        reg = '<.*?>|&nbsp'\n",
    "        content = re.sub(reg, \"\", res.text)                #清理内容，去掉所有HTML标签\n",
    "        reg_float = '\\d+\\.\\d+'                             #匹配小数，/d+:任意0-多位数字；/.：匹配小数点\n",
    "        \n",
    "        #南方\n",
    "        reg_south = '(南方省份哨点医院|南方省份2005年设立的哨点医院).*?[，]'              #a.*?b:匹配一长段字符中以a开头以b结尾的子串\n",
    "        south = re.search(reg_south,content).group()                             \n",
    "        S_ILI = re.search(reg_float, south).group()\n",
    "        s_ili = float(S_ILI)\n",
    "        \n",
    "        #北方\n",
    "        reg_north = '(北方省份哨点医院|北方省份内科监测诊室|北方省份2005年设立的哨点医院).*?[，]'         \n",
    "        north = re.search(reg_north,content).group()                          \n",
    "        N_ILI = re.search(reg_float, north).group()\n",
    "        n_ili = float(N_ILI)\n",
    "        \n",
    "        ILIdata.append([date, s_ili, n_ili])\n",
    "     \n",
    "    except Exception as e:\n",
    "        print(v)\n",
    "        print('except:', e)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ILIdata_df = pd.DataFrame(ILIdata, columns = [\"time\",\"south_ILI\",\"north_ILI\"])\n",
    "writer = pd.ExcelWriter(\"C:\\\\D\\\\HUST\\\\research_flu_forecast\\\\influenza data\\\\ILI_201001-202012\\\\ILI_202012.xlsx\")\n",
    "ILIdata_df.to_excel(writer,\"sheet1\",index = False)\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
